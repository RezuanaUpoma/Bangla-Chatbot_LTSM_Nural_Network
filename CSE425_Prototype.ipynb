{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAHFYw0P_nAd",
        "outputId": "79238c31-9692-48a7-94b9-f271bd7a5e87"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAQ-LsQ59Sin"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YXDc3qy_d5e"
      },
      "source": [
        "# **Preprocessing Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKjhvUNZ_n4B",
        "outputId": "39766519-cacd-4734-ca61-8cff25ccf17a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New vocab_size: 76\n",
            "Length of encoder_input_data: 301\n",
            "Length of decoder_target_data: 301\n",
            "Length of encoder_input_data: 301\n",
            "Length of decoder_target_data: 301\n",
            "240 61 240 61 240 61 240 61\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "# Function to read a text file and return its content as a list of lines\n",
        "# def read_text_file(file_path):\n",
        "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
        "#         content = file.readlines()\n",
        "#     return content\n",
        "\n",
        "# # Example usage:\n",
        "\n",
        "# conversational_data = read_text_file('conversational_dataset.txt')\n",
        "\n",
        "# # Vocabulary dataset\n",
        "# words = [line.strip() for line in vocabulary_data]\n",
        "\n",
        "# # Conversational dataset\n",
        "# # Separate questions and answers\n",
        "# questions = conversational_data[0::2]\n",
        "# answers = conversational_data[1::2]\n",
        "# print(questions)\n",
        "# print(answers)\n",
        "#   # Add start and end tokens to the answers\n",
        "\n",
        "\n",
        "\n",
        "def read_text_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.readlines()\n",
        "    return content\n",
        "\n",
        "vocabulary_data = read_text_file('বাংলা_শব্দ_তালিকা.txt')\n",
        "conversational_data = read_text_file('conversational_dataset1.txt')\n",
        "\n",
        "words = [line.strip() for line in vocabulary_data]\n",
        "\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for i in range(0, len(conversational_data), 3):  # Skipping every third line (the empty line)\n",
        "    question_line = conversational_data[i].strip()\n",
        "    answer_line = conversational_data[i + 1].strip()\n",
        "\n",
        "    # # Check if the line is a valid question-answer pair\n",
        "    # if question_line.endswith('?') and answer_line.endswith('।'):\n",
        "    #     questions.append(question_line)\n",
        "    #     answers.append(\"\\t \" + answer_line + \" \\n\")\n",
        "\n",
        "    questions.append(question_line)\n",
        "    answers.append(\"\\t \" + answer_line + \" \\n\")\n",
        "# print(questions)\n",
        "# print(answers)\n",
        "\n",
        "questions = [q.strip() for q in questions]\n",
        "answers = [\"\\t \" + a.strip() + \" \\n\" for a in answers]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Tokenize the words using the vocabulary dataset\n",
        "# # Add the start token explicitly to your vocabulary list\n",
        "# words.append('\\t')\n",
        "# tokenizer = Tokenizer(char_level=True)  # Char-level tokenization for languages like Bengali\n",
        "# tokenizer.fit_on_texts(words)\n",
        "\n",
        "# # Check if the start token index is now valid\n",
        "# start_token_index = tokenizer.word_index.get('\\t', None)\n",
        "# if start_token_index is None or start_token_index >= vocab_size:\n",
        "#     raise ValueError(\"Start token index is still invalid. Consider increasing vocab_size.\")\n",
        "\n",
        "# Fit tokenizer with your dataset\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(words + ['\\t'])  # Including the start token\n",
        "\n",
        "# Determine the new vocab_size\n",
        "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding token\n",
        "print(\"New vocab_size:\", vocab_size)\n",
        "\n",
        "\n",
        "# # Step 2: Tokenize the words using the vocabulary dataset\n",
        "# tokenizer = Tokenizer(char_level=True)  # Char-level tokenization for languages like Bengali\n",
        "# tokenizer.fit_on_texts(words)\n",
        "\n",
        "# # Step 2.1: Re-index the tokenizer (if necessary)\n",
        "# original_word_index = tokenizer.word_index\n",
        "# new_word_index = {}\n",
        "# new_index = 1  # Start from 1 since 0 is usually reserved for padding\n",
        "\n",
        "# for word, index in original_word_index.items():\n",
        "#     if index < vocab_size:\n",
        "#         new_word_index[word] = index\n",
        "#     else:\n",
        "#         new_word_index[word] = new_index\n",
        "#         new_index += 1\n",
        "\n",
        "# # Update tokenizer's word index\n",
        "# tokenizer.word_index = new_word_index\n",
        "# tokenizer.index_word = {index: word for word, index in new_word_index.items()}\n",
        "\n",
        "# # Check if the start token index is now valid\n",
        "# start_token_index = tokenizer.word_index.get('\\t', None)\n",
        "# if start_token_index is None or start_token_index >= vocab_size:\n",
        "#     raise ValueError(\"Start token index is still invalid after re-indexing.\")\n",
        "\n",
        "# Step 3: Convert questions and answers to sequences\n",
        "question_sequences = tokenizer.texts_to_sequences(questions)\n",
        "answer_sequences = tokenizer.texts_to_sequences(answers)\n",
        "\n",
        "# Step 4: Pad sequences to ensure uniform length\n",
        "max_len_questions = max([len(seq) for seq in question_sequences])\n",
        "max_len_answers = max([len(seq) for seq in answer_sequences])\n",
        "\n",
        "encoder_input_data = pad_sequences(question_sequences, maxlen=max_len_questions, padding='post')\n",
        "decoder_input_data = pad_sequences(answer_sequences, maxlen=max_len_answers, padding='post')\n",
        "\n",
        "# Step 5: Prepare decoder target data\n",
        "# The decoder_target_data is ahead of decoder_input_data by one timestep and will not include the start token.\n",
        "decoder_target_data = []\n",
        "for seq in answer_sequences:\n",
        "    decoder_target_data.append(seq[1:])  # Skip the start token\n",
        "\n",
        "decoder_target_data = pad_sequences(decoder_target_data, maxlen=max_len_answers, padding='post')\n",
        "\n",
        "# Check the length of encoder_input_data and decoder_target_data\n",
        "print(\"Length of encoder_input_data:\", len(encoder_input_data))\n",
        "print(\"Length of decoder_target_data:\", len(decoder_target_data))\n",
        "\n",
        "# Ensure they have the same length\n",
        "if len(encoder_input_data) != len(decoder_target_data):\n",
        "    min_length = min(len(encoder_input_data), len(decoder_target_data))\n",
        "    encoder_input_data = encoder_input_data[:min_length]\n",
        "    decoder_target_data = decoder_target_data[:min_length]\n",
        "\n",
        "print(\"Length of encoder_input_data:\", len(encoder_input_data))\n",
        "print(\"Length of decoder_target_data:\", len(decoder_target_data))\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "validation_split = 0.2\n",
        "(train_encoder_input_data, val_encoder_input_data,\n",
        " train_decoder_input_data, val_decoder_input_data,\n",
        " train_decoder_target_data, val_decoder_target_data) = train_test_split(\n",
        "    encoder_input_data, decoder_input_data, decoder_target_data,\n",
        "    test_size=validation_split, random_state=42)\n",
        "\n",
        "# # Now you can fit the model\n",
        "# # Make sure you also have the decoder_input_data which is required for Seq2Seq model\n",
        "# model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=64, epochs=100)\n",
        "\n",
        "# Step 6: One-hot encode the decoder target data\n",
        "train_decoder_target_data_one_hot = tf.keras.utils.to_categorical(train_decoder_target_data, len(tokenizer.word_index) + 1)\n",
        "val_decoder_target_data_one_hot = tf.keras.utils.to_categorical(val_decoder_target_data, len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Now, encoder_input_data, decoder_input_data, and decoder_target_data_one_hot are ready for model training\n",
        "print(len(train_encoder_input_data), len(val_encoder_input_data),\n",
        " len(train_decoder_input_data), len(val_decoder_input_data),\n",
        " len(train_decoder_target_data), len(val_decoder_target_data), len(train_decoder_target_data_one_hot), len(val_decoder_target_data_one_hot))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtfAMLqf_1oq"
      },
      "source": [
        "# **Training Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5Hu98dlH__iu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "8/8 [==============================] - 14s 1s/step - loss: 3.2732 - accuracy: 0.5988 - val_loss: 1.8227 - val_accuracy: 0.6702\n",
            "Epoch 2/200\n",
            "8/8 [==============================] - 8s 995ms/step - loss: 1.5787 - accuracy: 0.6817 - val_loss: 1.5975 - val_accuracy: 0.6651\n",
            "Epoch 3/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 1.4819 - accuracy: 0.6818 - val_loss: 1.5537 - val_accuracy: 0.6631\n",
            "Epoch 4/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 1.4959 - accuracy: 0.6810 - val_loss: 1.5279 - val_accuracy: 0.6619\n",
            "Epoch 5/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 1.4251 - accuracy: 0.6798 - val_loss: 1.4650 - val_accuracy: 0.6604\n",
            "Epoch 6/200\n",
            "8/8 [==============================] - 9s 1s/step - loss: 1.3901 - accuracy: 0.6791 - val_loss: 1.5366 - val_accuracy: 0.6639\n",
            "Epoch 7/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 1.4619 - accuracy: 0.6889 - val_loss: 1.4798 - val_accuracy: 0.6733\n",
            "Epoch 8/200\n",
            "8/8 [==============================] - 8s 985ms/step - loss: 1.3761 - accuracy: 0.6915 - val_loss: 1.4140 - val_accuracy: 0.6736\n",
            "Epoch 9/200\n",
            "8/8 [==============================] - 9s 1s/step - loss: 1.2851 - accuracy: 0.6911 - val_loss: 1.2690 - val_accuracy: 0.6737\n",
            "Epoch 10/200\n",
            "8/8 [==============================] - 9s 1s/step - loss: 1.1571 - accuracy: 0.7105 - val_loss: 1.2077 - val_accuracy: 0.6985\n",
            "Epoch 11/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 1.1273 - accuracy: 0.7213 - val_loss: 1.1875 - val_accuracy: 0.7079\n",
            "Epoch 12/200\n",
            "8/8 [==============================] - 8s 949ms/step - loss: 1.1152 - accuracy: 0.7234 - val_loss: 1.1801 - val_accuracy: 0.7046\n",
            "Epoch 13/200\n",
            "8/8 [==============================] - 7s 938ms/step - loss: 1.1078 - accuracy: 0.7232 - val_loss: 1.1730 - val_accuracy: 0.7055\n",
            "Epoch 14/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 1.1021 - accuracy: 0.7226 - val_loss: 1.1694 - val_accuracy: 0.7068\n",
            "Epoch 15/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 1.0975 - accuracy: 0.7241 - val_loss: 1.1642 - val_accuracy: 0.7066\n",
            "Epoch 16/200\n",
            "8/8 [==============================] - 8s 992ms/step - loss: 1.0932 - accuracy: 0.7241 - val_loss: 1.1605 - val_accuracy: 0.7075\n",
            "Epoch 17/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 1.0891 - accuracy: 0.7253 - val_loss: 1.1571 - val_accuracy: 0.7077\n",
            "Epoch 18/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 1.0855 - accuracy: 0.7264 - val_loss: 1.1529 - val_accuracy: 0.7094\n",
            "Epoch 19/200\n",
            "8/8 [==============================] - 8s 983ms/step - loss: 1.0818 - accuracy: 0.7263 - val_loss: 1.1501 - val_accuracy: 0.7090\n",
            "Epoch 20/200\n",
            "8/8 [==============================] - 8s 984ms/step - loss: 1.0779 - accuracy: 0.7266 - val_loss: 1.1465 - val_accuracy: 0.7089\n",
            "Epoch 21/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 1.0742 - accuracy: 0.7270 - val_loss: 1.1420 - val_accuracy: 0.7095\n",
            "Epoch 22/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 1.0702 - accuracy: 0.7272 - val_loss: 1.1380 - val_accuracy: 0.7094\n",
            "Epoch 23/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 1.0654 - accuracy: 0.7271 - val_loss: 1.1336 - val_accuracy: 0.7102\n",
            "Epoch 24/200\n",
            "8/8 [==============================] - 8s 995ms/step - loss: 1.0605 - accuracy: 0.7277 - val_loss: 1.1288 - val_accuracy: 0.7097\n",
            "Epoch 25/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 1.0550 - accuracy: 0.7277 - val_loss: 1.1234 - val_accuracy: 0.7104\n",
            "Epoch 26/200\n",
            "8/8 [==============================] - 8s 981ms/step - loss: 1.0497 - accuracy: 0.7280 - val_loss: 1.1165 - val_accuracy: 0.7104\n",
            "Epoch 27/200\n",
            "8/8 [==============================] - 8s 974ms/step - loss: 1.0425 - accuracy: 0.7289 - val_loss: 1.1092 - val_accuracy: 0.7121\n",
            "Epoch 28/200\n",
            "8/8 [==============================] - 8s 980ms/step - loss: 1.0345 - accuracy: 0.7309 - val_loss: 1.1007 - val_accuracy: 0.7144\n",
            "Epoch 29/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 1.0236 - accuracy: 0.7376 - val_loss: 1.0887 - val_accuracy: 0.7193\n",
            "Epoch 30/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 1.0141 - accuracy: 0.7403 - val_loss: 1.0788 - val_accuracy: 0.7237\n",
            "Epoch 31/200\n",
            "8/8 [==============================] - 8s 997ms/step - loss: 1.0020 - accuracy: 0.7439 - val_loss: 1.0662 - val_accuracy: 0.7268\n",
            "Epoch 32/200\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.9895 - accuracy: 0.7467 - val_loss: 1.0528 - val_accuracy: 0.7276\n",
            "Epoch 33/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.9776 - accuracy: 0.7485 - val_loss: 1.0437 - val_accuracy: 0.7268\n",
            "Epoch 34/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.9666 - accuracy: 0.7493 - val_loss: 1.0312 - val_accuracy: 0.7319\n",
            "Epoch 35/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.9541 - accuracy: 0.7515 - val_loss: 1.0169 - val_accuracy: 0.7302\n",
            "Epoch 36/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.9415 - accuracy: 0.7527 - val_loss: 1.0087 - val_accuracy: 0.7318\n",
            "Epoch 37/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.9306 - accuracy: 0.7553 - val_loss: 0.9961 - val_accuracy: 0.7356\n",
            "Epoch 38/200\n",
            "8/8 [==============================] - 8s 955ms/step - loss: 0.9212 - accuracy: 0.7551 - val_loss: 0.9883 - val_accuracy: 0.7355\n",
            "Epoch 39/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.9116 - accuracy: 0.7578 - val_loss: 0.9784 - val_accuracy: 0.7350\n",
            "Epoch 40/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.9041 - accuracy: 0.7586 - val_loss: 0.9713 - val_accuracy: 0.7411\n",
            "Epoch 41/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.8959 - accuracy: 0.7593 - val_loss: 0.9638 - val_accuracy: 0.7396\n",
            "Epoch 42/200\n",
            "8/8 [==============================] - 8s 989ms/step - loss: 0.8877 - accuracy: 0.7629 - val_loss: 0.9591 - val_accuracy: 0.7361\n",
            "Epoch 43/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.8824 - accuracy: 0.7617 - val_loss: 0.9503 - val_accuracy: 0.7451\n",
            "Epoch 44/200\n",
            "8/8 [==============================] - 8s 999ms/step - loss: 0.8764 - accuracy: 0.7637 - val_loss: 0.9451 - val_accuracy: 0.7412\n",
            "Epoch 45/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.8699 - accuracy: 0.7642 - val_loss: 0.9387 - val_accuracy: 0.7441\n",
            "Epoch 46/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.8653 - accuracy: 0.7669 - val_loss: 0.9323 - val_accuracy: 0.7472\n",
            "Epoch 47/200\n",
            "8/8 [==============================] - 8s 1000ms/step - loss: 0.8587 - accuracy: 0.7693 - val_loss: 0.9268 - val_accuracy: 0.7505\n",
            "Epoch 48/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.8541 - accuracy: 0.7706 - val_loss: 0.9250 - val_accuracy: 0.7457\n",
            "Epoch 49/200\n",
            "8/8 [==============================] - 8s 979ms/step - loss: 0.8489 - accuracy: 0.7705 - val_loss: 0.9177 - val_accuracy: 0.7514\n",
            "Epoch 50/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.8446 - accuracy: 0.7723 - val_loss: 0.9109 - val_accuracy: 0.7537\n",
            "Epoch 51/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.8382 - accuracy: 0.7742 - val_loss: 0.9064 - val_accuracy: 0.7540\n",
            "Epoch 52/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.8349 - accuracy: 0.7747 - val_loss: 0.9027 - val_accuracy: 0.7530\n",
            "Epoch 53/200\n",
            "8/8 [==============================] - 8s 985ms/step - loss: 0.8298 - accuracy: 0.7745 - val_loss: 0.8973 - val_accuracy: 0.7549\n",
            "Epoch 54/200\n",
            "8/8 [==============================] - 8s 979ms/step - loss: 0.8258 - accuracy: 0.7763 - val_loss: 0.8928 - val_accuracy: 0.7533\n",
            "Epoch 55/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.8223 - accuracy: 0.7768 - val_loss: 0.8898 - val_accuracy: 0.7543\n",
            "Epoch 56/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.8192 - accuracy: 0.7776 - val_loss: 0.8860 - val_accuracy: 0.7554\n",
            "Epoch 57/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.8160 - accuracy: 0.7775 - val_loss: 0.8838 - val_accuracy: 0.7560\n",
            "Epoch 58/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.8125 - accuracy: 0.7775 - val_loss: 0.8779 - val_accuracy: 0.7595\n",
            "Epoch 59/200\n",
            "8/8 [==============================] - 8s 978ms/step - loss: 0.8072 - accuracy: 0.7795 - val_loss: 0.8745 - val_accuracy: 0.7589\n",
            "Epoch 60/200\n",
            "8/8 [==============================] - 8s 978ms/step - loss: 0.8027 - accuracy: 0.7809 - val_loss: 0.8743 - val_accuracy: 0.7580\n",
            "Epoch 61/200\n",
            "8/8 [==============================] - 8s 971ms/step - loss: 0.8006 - accuracy: 0.7812 - val_loss: 0.8676 - val_accuracy: 0.7607\n",
            "Epoch 62/200\n",
            "8/8 [==============================] - 8s 999ms/step - loss: 0.7967 - accuracy: 0.7818 - val_loss: 0.8641 - val_accuracy: 0.7612\n",
            "Epoch 63/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.7928 - accuracy: 0.7828 - val_loss: 0.8615 - val_accuracy: 0.7629\n",
            "Epoch 64/200\n",
            "8/8 [==============================] - 8s 956ms/step - loss: 0.7912 - accuracy: 0.7821 - val_loss: 0.8597 - val_accuracy: 0.7623\n",
            "Epoch 65/200\n",
            "8/8 [==============================] - 8s 971ms/step - loss: 0.7875 - accuracy: 0.7836 - val_loss: 0.8585 - val_accuracy: 0.7622\n",
            "Epoch 66/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.7855 - accuracy: 0.7837 - val_loss: 0.8553 - val_accuracy: 0.7628\n",
            "Epoch 67/200\n",
            "8/8 [==============================] - 8s 975ms/step - loss: 0.7805 - accuracy: 0.7857 - val_loss: 0.8515 - val_accuracy: 0.7642\n",
            "Epoch 68/200\n",
            "8/8 [==============================] - 7s 938ms/step - loss: 0.7782 - accuracy: 0.7860 - val_loss: 0.8481 - val_accuracy: 0.7659\n",
            "Epoch 69/200\n",
            "8/8 [==============================] - 7s 912ms/step - loss: 0.7751 - accuracy: 0.7868 - val_loss: 0.8473 - val_accuracy: 0.7662\n",
            "Epoch 70/200\n",
            "8/8 [==============================] - 8s 948ms/step - loss: 0.7740 - accuracy: 0.7858 - val_loss: 0.8454 - val_accuracy: 0.7657\n",
            "Epoch 71/200\n",
            "8/8 [==============================] - 7s 930ms/step - loss: 0.7689 - accuracy: 0.7881 - val_loss: 0.8414 - val_accuracy: 0.7669\n",
            "Epoch 72/200\n",
            "8/8 [==============================] - 7s 918ms/step - loss: 0.7681 - accuracy: 0.7872 - val_loss: 0.8393 - val_accuracy: 0.7671\n",
            "Epoch 73/200\n",
            "8/8 [==============================] - 7s 918ms/step - loss: 0.7638 - accuracy: 0.7893 - val_loss: 0.8367 - val_accuracy: 0.7695\n",
            "Epoch 74/200\n",
            "8/8 [==============================] - 7s 915ms/step - loss: 0.7624 - accuracy: 0.7898 - val_loss: 0.8393 - val_accuracy: 0.7690\n",
            "Epoch 75/200\n",
            "8/8 [==============================] - 7s 920ms/step - loss: 0.7607 - accuracy: 0.7892 - val_loss: 0.8339 - val_accuracy: 0.7676\n",
            "Epoch 76/200\n",
            "8/8 [==============================] - 7s 918ms/step - loss: 0.7568 - accuracy: 0.7910 - val_loss: 0.8322 - val_accuracy: 0.7674\n",
            "Epoch 77/200\n",
            "8/8 [==============================] - 7s 915ms/step - loss: 0.7548 - accuracy: 0.7916 - val_loss: 0.8299 - val_accuracy: 0.7687\n",
            "Epoch 78/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.7526 - accuracy: 0.7913 - val_loss: 0.8266 - val_accuracy: 0.7690\n",
            "Epoch 79/200\n",
            "8/8 [==============================] - 8s 994ms/step - loss: 0.7491 - accuracy: 0.7924 - val_loss: 0.8262 - val_accuracy: 0.7706\n",
            "Epoch 80/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.7473 - accuracy: 0.7925 - val_loss: 0.8221 - val_accuracy: 0.7713\n",
            "Epoch 81/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.7433 - accuracy: 0.7939 - val_loss: 0.8219 - val_accuracy: 0.7700\n",
            "Epoch 82/200\n",
            "8/8 [==============================] - 8s 996ms/step - loss: 0.7405 - accuracy: 0.7941 - val_loss: 0.8189 - val_accuracy: 0.7719\n",
            "Epoch 83/200\n",
            "8/8 [==============================] - 8s 976ms/step - loss: 0.7393 - accuracy: 0.7943 - val_loss: 0.8198 - val_accuracy: 0.7716\n",
            "Epoch 84/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.7371 - accuracy: 0.7955 - val_loss: 0.8147 - val_accuracy: 0.7722\n",
            "Epoch 85/200\n",
            "8/8 [==============================] - 8s 950ms/step - loss: 0.7346 - accuracy: 0.7956 - val_loss: 0.8127 - val_accuracy: 0.7744\n",
            "Epoch 86/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.7310 - accuracy: 0.7969 - val_loss: 0.8121 - val_accuracy: 0.7744\n",
            "Epoch 87/200\n",
            "8/8 [==============================] - 8s 975ms/step - loss: 0.7292 - accuracy: 0.7971 - val_loss: 0.8090 - val_accuracy: 0.7740\n",
            "Epoch 88/200\n",
            "8/8 [==============================] - 8s 980ms/step - loss: 0.7264 - accuracy: 0.7975 - val_loss: 0.8089 - val_accuracy: 0.7751\n",
            "Epoch 89/200\n",
            "8/8 [==============================] - 8s 975ms/step - loss: 0.7216 - accuracy: 0.7984 - val_loss: 0.8065 - val_accuracy: 0.7756\n",
            "Epoch 90/200\n",
            "8/8 [==============================] - 8s 988ms/step - loss: 0.7195 - accuracy: 0.7996 - val_loss: 0.8038 - val_accuracy: 0.7747\n",
            "Epoch 91/200\n",
            "8/8 [==============================] - 8s 976ms/step - loss: 0.7160 - accuracy: 0.8004 - val_loss: 0.8025 - val_accuracy: 0.7764\n",
            "Epoch 92/200\n",
            "8/8 [==============================] - 8s 966ms/step - loss: 0.7135 - accuracy: 0.8015 - val_loss: 0.8021 - val_accuracy: 0.7769\n",
            "Epoch 93/200\n",
            "8/8 [==============================] - 8s 968ms/step - loss: 0.7101 - accuracy: 0.8018 - val_loss: 0.8051 - val_accuracy: 0.7756\n",
            "Epoch 94/200\n",
            "8/8 [==============================] - 8s 991ms/step - loss: 0.7099 - accuracy: 0.8009 - val_loss: 0.7990 - val_accuracy: 0.7771\n",
            "Epoch 95/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.7061 - accuracy: 0.8027 - val_loss: 0.7999 - val_accuracy: 0.7791\n",
            "Epoch 96/200\n",
            "8/8 [==============================] - 8s 987ms/step - loss: 0.7045 - accuracy: 0.8026 - val_loss: 0.7967 - val_accuracy: 0.7793\n",
            "Epoch 97/200\n",
            "8/8 [==============================] - 8s 970ms/step - loss: 0.7033 - accuracy: 0.8030 - val_loss: 0.7953 - val_accuracy: 0.7770\n",
            "Epoch 98/200\n",
            "8/8 [==============================] - 8s 953ms/step - loss: 0.6990 - accuracy: 0.8040 - val_loss: 0.7927 - val_accuracy: 0.7790\n",
            "Epoch 99/200\n",
            "8/8 [==============================] - 8s 959ms/step - loss: 0.6956 - accuracy: 0.8052 - val_loss: 0.7907 - val_accuracy: 0.7791\n",
            "Epoch 100/200\n",
            "8/8 [==============================] - 8s 992ms/step - loss: 0.6928 - accuracy: 0.8049 - val_loss: 0.7929 - val_accuracy: 0.7798\n",
            "Epoch 101/200\n",
            "8/8 [==============================] - 8s 989ms/step - loss: 0.6895 - accuracy: 0.8064 - val_loss: 0.7890 - val_accuracy: 0.7795\n",
            "Epoch 102/200\n",
            "8/8 [==============================] - 8s 976ms/step - loss: 0.6872 - accuracy: 0.8069 - val_loss: 0.7918 - val_accuracy: 0.7799\n",
            "Epoch 103/200\n",
            "8/8 [==============================] - 8s 956ms/step - loss: 0.6842 - accuracy: 0.8085 - val_loss: 0.7839 - val_accuracy: 0.7800\n",
            "Epoch 104/200\n",
            "8/8 [==============================] - 8s 963ms/step - loss: 0.6798 - accuracy: 0.8102 - val_loss: 0.7841 - val_accuracy: 0.7808\n",
            "Epoch 105/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.6775 - accuracy: 0.8099 - val_loss: 0.7837 - val_accuracy: 0.7819\n",
            "Epoch 106/200\n",
            "8/8 [==============================] - 8s 981ms/step - loss: 0.6733 - accuracy: 0.8112 - val_loss: 0.7843 - val_accuracy: 0.7819\n",
            "Epoch 107/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.6726 - accuracy: 0.8110 - val_loss: 0.7807 - val_accuracy: 0.7824\n",
            "Epoch 108/200\n",
            "8/8 [==============================] - 8s 981ms/step - loss: 0.6680 - accuracy: 0.8123 - val_loss: 0.7780 - val_accuracy: 0.7831\n",
            "Epoch 109/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.6659 - accuracy: 0.8122 - val_loss: 0.7771 - val_accuracy: 0.7828\n",
            "Epoch 110/200\n",
            "8/8 [==============================] - 8s 999ms/step - loss: 0.6634 - accuracy: 0.8140 - val_loss: 0.7780 - val_accuracy: 0.7830\n",
            "Epoch 111/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.6605 - accuracy: 0.8150 - val_loss: 0.7771 - val_accuracy: 0.7839\n",
            "Epoch 112/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.6571 - accuracy: 0.8152 - val_loss: 0.7774 - val_accuracy: 0.7828\n",
            "Epoch 113/200\n",
            "8/8 [==============================] - 8s 985ms/step - loss: 0.6539 - accuracy: 0.8159 - val_loss: 0.7730 - val_accuracy: 0.7858\n",
            "Epoch 114/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.6506 - accuracy: 0.8173 - val_loss: 0.7722 - val_accuracy: 0.7833\n",
            "Epoch 115/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.6481 - accuracy: 0.8179 - val_loss: 0.7745 - val_accuracy: 0.7852\n",
            "Epoch 116/200\n",
            "8/8 [==============================] - 8s 996ms/step - loss: 0.6455 - accuracy: 0.8186 - val_loss: 0.7822 - val_accuracy: 0.7839\n",
            "Epoch 117/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.6438 - accuracy: 0.8187 - val_loss: 0.7744 - val_accuracy: 0.7849\n",
            "Epoch 118/200\n",
            "8/8 [==============================] - 8s 957ms/step - loss: 0.6398 - accuracy: 0.8193 - val_loss: 0.7735 - val_accuracy: 0.7853\n",
            "Epoch 119/200\n",
            "8/8 [==============================] - 7s 921ms/step - loss: 0.6349 - accuracy: 0.8217 - val_loss: 0.7697 - val_accuracy: 0.7881\n",
            "Epoch 120/200\n",
            "8/8 [==============================] - 7s 917ms/step - loss: 0.6305 - accuracy: 0.8222 - val_loss: 0.7689 - val_accuracy: 0.7866\n",
            "Epoch 121/200\n",
            "8/8 [==============================] - 7s 930ms/step - loss: 0.6251 - accuracy: 0.8241 - val_loss: 0.7698 - val_accuracy: 0.7871\n",
            "Epoch 122/200\n",
            "8/8 [==============================] - 7s 912ms/step - loss: 0.6216 - accuracy: 0.8254 - val_loss: 0.7691 - val_accuracy: 0.7867\n",
            "Epoch 123/200\n",
            "8/8 [==============================] - 7s 915ms/step - loss: 0.6175 - accuracy: 0.8267 - val_loss: 0.7720 - val_accuracy: 0.7847\n",
            "Epoch 124/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.6156 - accuracy: 0.8266 - val_loss: 0.7683 - val_accuracy: 0.7883\n",
            "Epoch 125/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.6123 - accuracy: 0.8276 - val_loss: 0.7718 - val_accuracy: 0.7863\n",
            "Epoch 126/200\n",
            "8/8 [==============================] - 8s 1000ms/step - loss: 0.6094 - accuracy: 0.8283 - val_loss: 0.7718 - val_accuracy: 0.7881\n",
            "Epoch 127/200\n",
            "8/8 [==============================] - 7s 920ms/step - loss: 0.6043 - accuracy: 0.8299 - val_loss: 0.7705 - val_accuracy: 0.7887\n",
            "Epoch 128/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.6035 - accuracy: 0.8302 - val_loss: 0.7654 - val_accuracy: 0.7896\n",
            "Epoch 129/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.5977 - accuracy: 0.8310 - val_loss: 0.7673 - val_accuracy: 0.7894\n",
            "Epoch 130/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.5908 - accuracy: 0.8342 - val_loss: 0.7723 - val_accuracy: 0.7886\n",
            "Epoch 131/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.5879 - accuracy: 0.8342 - val_loss: 0.7687 - val_accuracy: 0.7882\n",
            "Epoch 132/200\n",
            "8/8 [==============================] - 8s 965ms/step - loss: 0.5836 - accuracy: 0.8358 - val_loss: 0.7715 - val_accuracy: 0.7906\n",
            "Epoch 133/200\n",
            "8/8 [==============================] - 7s 935ms/step - loss: 0.5817 - accuracy: 0.8367 - val_loss: 0.7708 - val_accuracy: 0.7885\n",
            "Epoch 134/200\n",
            "8/8 [==============================] - 7s 907ms/step - loss: 0.5787 - accuracy: 0.8372 - val_loss: 0.7705 - val_accuracy: 0.7886\n",
            "Epoch 135/200\n",
            "8/8 [==============================] - 7s 908ms/step - loss: 0.5752 - accuracy: 0.8380 - val_loss: 0.7706 - val_accuracy: 0.7882\n",
            "Epoch 136/200\n",
            "8/8 [==============================] - 7s 915ms/step - loss: 0.5686 - accuracy: 0.8405 - val_loss: 0.7733 - val_accuracy: 0.7893\n",
            "Epoch 137/200\n",
            "8/8 [==============================] - 7s 922ms/step - loss: 0.5662 - accuracy: 0.8402 - val_loss: 0.7704 - val_accuracy: 0.7883\n",
            "Epoch 138/200\n",
            "8/8 [==============================] - 7s 925ms/step - loss: 0.5605 - accuracy: 0.8413 - val_loss: 0.7704 - val_accuracy: 0.7894\n",
            "Epoch 139/200\n",
            "8/8 [==============================] - 7s 913ms/step - loss: 0.5565 - accuracy: 0.8427 - val_loss: 0.7760 - val_accuracy: 0.7879\n",
            "Epoch 140/200\n",
            "8/8 [==============================] - 7s 916ms/step - loss: 0.5507 - accuracy: 0.8460 - val_loss: 0.7720 - val_accuracy: 0.7905\n",
            "Epoch 141/200\n",
            "8/8 [==============================] - 7s 918ms/step - loss: 0.5453 - accuracy: 0.8471 - val_loss: 0.7717 - val_accuracy: 0.7895\n",
            "Epoch 142/200\n",
            "8/8 [==============================] - 7s 917ms/step - loss: 0.5422 - accuracy: 0.8463 - val_loss: 0.7758 - val_accuracy: 0.7881\n",
            "Epoch 143/200\n",
            "8/8 [==============================] - 7s 923ms/step - loss: 0.5380 - accuracy: 0.8475 - val_loss: 0.7706 - val_accuracy: 0.7898\n",
            "Epoch 144/200\n",
            "8/8 [==============================] - 7s 919ms/step - loss: 0.5329 - accuracy: 0.8494 - val_loss: 0.7748 - val_accuracy: 0.7883\n",
            "Epoch 145/200\n",
            "8/8 [==============================] - 7s 921ms/step - loss: 0.5284 - accuracy: 0.8513 - val_loss: 0.7759 - val_accuracy: 0.7882\n",
            "Epoch 146/200\n",
            "8/8 [==============================] - 7s 918ms/step - loss: 0.5211 - accuracy: 0.8526 - val_loss: 0.7795 - val_accuracy: 0.7892\n",
            "Epoch 147/200\n",
            "8/8 [==============================] - 7s 919ms/step - loss: 0.5169 - accuracy: 0.8545 - val_loss: 0.7763 - val_accuracy: 0.7894\n",
            "Epoch 148/200\n",
            "8/8 [==============================] - 7s 924ms/step - loss: 0.5133 - accuracy: 0.8546 - val_loss: 0.7790 - val_accuracy: 0.7893\n",
            "Epoch 149/200\n",
            "8/8 [==============================] - 7s 922ms/step - loss: 0.5070 - accuracy: 0.8565 - val_loss: 0.7809 - val_accuracy: 0.7897\n",
            "Epoch 150/200\n",
            "8/8 [==============================] - 7s 913ms/step - loss: 0.5067 - accuracy: 0.8578 - val_loss: 0.7837 - val_accuracy: 0.7883\n",
            "Epoch 151/200\n",
            "8/8 [==============================] - 7s 922ms/step - loss: 0.5033 - accuracy: 0.8570 - val_loss: 0.7810 - val_accuracy: 0.7895\n",
            "Epoch 152/200\n",
            "8/8 [==============================] - 7s 916ms/step - loss: 0.4979 - accuracy: 0.8587 - val_loss: 0.7855 - val_accuracy: 0.7903\n",
            "Epoch 153/200\n",
            "8/8 [==============================] - 7s 922ms/step - loss: 0.4937 - accuracy: 0.8598 - val_loss: 0.7863 - val_accuracy: 0.7900\n",
            "Epoch 154/200\n",
            "8/8 [==============================] - 7s 918ms/step - loss: 0.4869 - accuracy: 0.8627 - val_loss: 0.7901 - val_accuracy: 0.7900\n",
            "Epoch 155/200\n",
            "8/8 [==============================] - 7s 920ms/step - loss: 0.4813 - accuracy: 0.8642 - val_loss: 0.7884 - val_accuracy: 0.7895\n",
            "Epoch 156/200\n",
            "8/8 [==============================] - 7s 917ms/step - loss: 0.4798 - accuracy: 0.8644 - val_loss: 0.7852 - val_accuracy: 0.7891\n",
            "Epoch 157/200\n",
            "8/8 [==============================] - 7s 913ms/step - loss: 0.4744 - accuracy: 0.8663 - val_loss: 0.7951 - val_accuracy: 0.7904\n",
            "Epoch 158/200\n",
            "8/8 [==============================] - 7s 916ms/step - loss: 0.4731 - accuracy: 0.8660 - val_loss: 0.7955 - val_accuracy: 0.7880\n",
            "Epoch 159/200\n",
            "8/8 [==============================] - 7s 920ms/step - loss: 0.4652 - accuracy: 0.8687 - val_loss: 0.7936 - val_accuracy: 0.7898\n",
            "Epoch 160/200\n",
            "8/8 [==============================] - 7s 921ms/step - loss: 0.4607 - accuracy: 0.8690 - val_loss: 0.7971 - val_accuracy: 0.7900\n",
            "Epoch 161/200\n",
            "8/8 [==============================] - 7s 918ms/step - loss: 0.4545 - accuracy: 0.8711 - val_loss: 0.7910 - val_accuracy: 0.7891\n",
            "Epoch 162/200\n",
            "8/8 [==============================] - 7s 922ms/step - loss: 0.4483 - accuracy: 0.8736 - val_loss: 0.8052 - val_accuracy: 0.7895\n",
            "Epoch 163/200\n",
            "8/8 [==============================] - 7s 923ms/step - loss: 0.4441 - accuracy: 0.8751 - val_loss: 0.7978 - val_accuracy: 0.7899\n",
            "Epoch 164/200\n",
            "8/8 [==============================] - 7s 918ms/step - loss: 0.4418 - accuracy: 0.8752 - val_loss: 0.8064 - val_accuracy: 0.7890\n",
            "Epoch 165/200\n",
            "8/8 [==============================] - 7s 926ms/step - loss: 0.4377 - accuracy: 0.8774 - val_loss: 0.8045 - val_accuracy: 0.7886\n",
            "Epoch 166/200\n",
            "8/8 [==============================] - 7s 914ms/step - loss: 0.4304 - accuracy: 0.8793 - val_loss: 0.8109 - val_accuracy: 0.7890\n",
            "Epoch 167/200\n",
            "8/8 [==============================] - 7s 917ms/step - loss: 0.4272 - accuracy: 0.8802 - val_loss: 0.8120 - val_accuracy: 0.7888\n",
            "Epoch 168/200\n",
            "8/8 [==============================] - 7s 925ms/step - loss: 0.4245 - accuracy: 0.8806 - val_loss: 0.8132 - val_accuracy: 0.7881\n",
            "Epoch 169/200\n",
            "8/8 [==============================] - 7s 922ms/step - loss: 0.4198 - accuracy: 0.8816 - val_loss: 0.8137 - val_accuracy: 0.7874\n",
            "Epoch 170/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.4108 - accuracy: 0.8849 - val_loss: 0.8189 - val_accuracy: 0.7879\n",
            "Epoch 171/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.4078 - accuracy: 0.8849 - val_loss: 0.8221 - val_accuracy: 0.7866\n",
            "Epoch 172/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.4006 - accuracy: 0.8879 - val_loss: 0.8225 - val_accuracy: 0.7870\n",
            "Epoch 173/200\n",
            "8/8 [==============================] - 8s 996ms/step - loss: 0.3976 - accuracy: 0.8885 - val_loss: 0.8337 - val_accuracy: 0.7871\n",
            "Epoch 174/200\n",
            "8/8 [==============================] - 8s 991ms/step - loss: 0.3964 - accuracy: 0.8889 - val_loss: 0.8314 - val_accuracy: 0.7864\n",
            "Epoch 175/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.3904 - accuracy: 0.8905 - val_loss: 0.8305 - val_accuracy: 0.7874\n",
            "Epoch 176/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.3835 - accuracy: 0.8933 - val_loss: 0.8344 - val_accuracy: 0.7871\n",
            "Epoch 177/200\n",
            "8/8 [==============================] - 8s 991ms/step - loss: 0.3854 - accuracy: 0.8921 - val_loss: 0.8389 - val_accuracy: 0.7871\n",
            "Epoch 178/200\n",
            "8/8 [==============================] - 8s 995ms/step - loss: 0.3782 - accuracy: 0.8935 - val_loss: 0.8384 - val_accuracy: 0.7871\n",
            "Epoch 179/200\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.3722 - accuracy: 0.8956 - val_loss: 0.8388 - val_accuracy: 0.7886\n",
            "Epoch 180/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.3676 - accuracy: 0.8961 - val_loss: 0.8417 - val_accuracy: 0.7885\n",
            "Epoch 181/200\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.3593 - accuracy: 0.9002 - val_loss: 0.8461 - val_accuracy: 0.7888\n",
            "Epoch 182/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.3558 - accuracy: 0.9002 - val_loss: 0.8482 - val_accuracy: 0.7873\n",
            "Epoch 183/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.3531 - accuracy: 0.9008 - val_loss: 0.8551 - val_accuracy: 0.7849\n",
            "Epoch 184/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.3487 - accuracy: 0.9021 - val_loss: 0.8558 - val_accuracy: 0.7851\n",
            "Epoch 185/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.3436 - accuracy: 0.9036 - val_loss: 0.8560 - val_accuracy: 0.7879\n",
            "Epoch 186/200\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.3420 - accuracy: 0.9043 - val_loss: 0.8590 - val_accuracy: 0.7893\n",
            "Epoch 187/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.3387 - accuracy: 0.9053 - val_loss: 0.8592 - val_accuracy: 0.7854\n",
            "Epoch 188/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.3330 - accuracy: 0.9072 - val_loss: 0.8639 - val_accuracy: 0.7859\n",
            "Epoch 189/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.3287 - accuracy: 0.9077 - val_loss: 0.8647 - val_accuracy: 0.7858\n",
            "Epoch 190/200\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.3230 - accuracy: 0.9104 - val_loss: 0.8740 - val_accuracy: 0.7855\n",
            "Epoch 191/200\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.3199 - accuracy: 0.9110 - val_loss: 0.8713 - val_accuracy: 0.7865\n",
            "Epoch 192/200\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.3172 - accuracy: 0.9120 - val_loss: 0.8800 - val_accuracy: 0.7864\n",
            "Epoch 193/200\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.3082 - accuracy: 0.9150 - val_loss: 0.8778 - val_accuracy: 0.7861\n",
            "Epoch 194/200\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.3059 - accuracy: 0.9154 - val_loss: 0.8875 - val_accuracy: 0.7848\n",
            "Epoch 195/200\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.3023 - accuracy: 0.9169 - val_loss: 0.8845 - val_accuracy: 0.7876\n",
            "Epoch 196/200\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.2982 - accuracy: 0.9186 - val_loss: 0.8867 - val_accuracy: 0.7859\n",
            "Epoch 197/200\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.2934 - accuracy: 0.9200 - val_loss: 0.8882 - val_accuracy: 0.7868\n",
            "Epoch 198/200\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.2897 - accuracy: 0.9215 - val_loss: 0.8989 - val_accuracy: 0.7846\n",
            "Epoch 199/200\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.2877 - accuracy: 0.9215 - val_loss: 0.8978 - val_accuracy: 0.7857\n",
            "Epoch 200/200\n",
            "8/8 [==============================] - 9s 1s/step - loss: 0.2871 - accuracy: 0.9217 - val_loss: 0.9011 - val_accuracy: 0.7859\n",
            "INFO:tensorflow:Assets written to: bangla_chatbot_model.keras3\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: bangla_chatbot_model.keras3\\assets\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "#from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Assuming `input_tensor` and `target_tensor` are processed data tensors\n",
        "# Assuming you have already fitted a tokenizer on your text data\n",
        "vocab_size = len(tokenizer.word_index) + 1  # +1 for the padding token\n",
        "\n",
        "\n",
        "# Create a Seq2Seq Model\n",
        "encoder_inputs = tf.keras.Input(shape=(None,))\n",
        "encoder_embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=256)(encoder_inputs)\n",
        "encoder_lstm = tf.keras.layers.LSTM(512, return_state=True, dropout=0.4)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "#encoder_outputs, state_h, state_c = tf.keras.layers.LSTM(1024, return_state=True)(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = tf.keras.Input(shape=(None,))\n",
        "decoder_embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=256)(decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True, dropout=0.4)\n",
        "#decoder_lstm = tf.keras.layers.LSTM(1024, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "\n",
        "# Compile & Train the Model\n",
        "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Assuming encoder_input_data, decoder_input_data, and decoder_target_data_one_hot are correctly defined\n",
        "# model.fit(\n",
        "#     [encoder_input_data, decoder_input_data],  # Input to the model: encoder and decoder input\n",
        "#     decoder_target_data_one_hot,              # Target data: One-hot encoded target sequences\n",
        "#     batch_size=64,\n",
        "#     epochs=100\n",
        "# )\n",
        "\n",
        "# Learning Rate Scheduler or Early Stopping\n",
        "#early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0005)\n",
        "\n",
        "# Create a checkpoint callback\n",
        "#checkpoint = ModelCheckpoint('model_checkpoint.keras', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "model.fit(\n",
        "    [train_encoder_input_data, train_decoder_input_data], \n",
        "    train_decoder_target_data_one_hot,\n",
        "    #callbacks=[checkpoint, early_stopping, reduce_lr],              \n",
        "    validation_data=([val_encoder_input_data, val_decoder_input_data], val_decoder_target_data_one_hot),\n",
        "    batch_size=32,\n",
        "    epochs=200 #using early stopping\n",
        ")\n",
        "\n",
        "# Assuming you have a validation set prepared\n",
        "# Split your data into training and validation sets\n",
        "# train_encoder_input_data, val_encoder_input_data, train_decoder_input_data, val_decoder_input_data, train_decoder_target_data, val_decoder_target_data\n",
        "\n",
        "# Save the trained model\n",
        "model.save('bangla_chatbot_model.keras3')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFocQma-AFnO"
      },
      "source": [
        "# **RESPONSE GENERATOR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "A_ZrJb7AAJdu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "আপনি কেমন আছেন?\n",
            "Tokenized sequence: [[31 12  5  4 55  7  6 13  5 55 31 37  6  5  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
            "1/1 [==============================] - 0s 362ms/step\n",
            "1/1 [==============================] - 0s 310ms/step\n",
            "Predicted token index: 55\n",
            "Decoded so far:  \n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicted token index: 31\n",
            "Decoded so far:  আ\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted token index: 12\n",
            "Decoded so far:  আপ\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 5\n",
            "Decoded so far:  আপন\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicted token index: 4\n",
            "Decoded so far:  আপনি\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 55\n",
            "Decoded so far:  আপনি \n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted token index: 7\n",
            "Decoded so far:  আপনি ক\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 4\n",
            "Decoded so far:  আপনি কি\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted token index: 55\n",
            "Decoded so far:  আপনি কি \n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 8\n",
            "Decoded so far:  আপনি কি ব\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicted token index: 4\n",
            "Decoded so far:  আপনি কি বি\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted token index: 25\n",
            "Decoded so far:  আপনি কি বিভ\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicted token index: 1\n",
            "Decoded so far:  আপনি কি বিভা\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 5\n",
            "Decoded so far:  আপনি কি বিভান\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted token index: 6\n",
            "Decoded so far:  আপনি কি বিভানে\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Predicted token index: 55\n",
            "Decoded so far:  আপনি কি বিভানে \n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicted token index: 5\n",
            "Decoded so far:  আপনি কি বিভানে ন\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted token index: 2\n",
            "Decoded so far:  আপনি কি বিভানে ন্\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 3\n",
            "Decoded so far:  আপনি কি বিভানে ন্র\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 8\n",
            "Decoded so far:  আপনি কি বিভানে ন্রব\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicted token index: 8\n",
            "Decoded so far:  আপনি কি বিভানে ন্রবব\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicted token index: 1\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববা\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 5\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 55\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান \n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicted token index: 8\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted token index: 2\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 11\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্য\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted token index: 8\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যব\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 1\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবা\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted token index: 3\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicted token index: 55\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার \n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted token index: 7\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার ক\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 3\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 2\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted token index: 37\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছ\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 6\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 55\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে \n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicted token index: 43\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এ\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 8\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এব\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicted token index: 38\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicted token index: 55\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং \n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicted token index: 43\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এ\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 20\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এট\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicted token index: 4\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicted token index: 55\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি \n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicted token index: 43\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি এ\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicted token index: 7\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি এক\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 20\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একট\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted token index: 4\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 55\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি \n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicted token index: 10\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি স\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicted token index: 16\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সু\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted token index: 13\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Predicted token index: 2\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Predicted token index: 11\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্য\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicted token index: 1\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যা\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Predicted token index: 5\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Predicted token index: 55\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান \n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 7\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান ক\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted token index: 3\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicted token index: 2\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicted token index: 12\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্প\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted token index: 1\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পা\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Predicted token index: 3\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পার\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicted token index: 6\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicted token index: 55\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে \n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicted token index: 43\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এ\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 8\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এব\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Predicted token index: 38\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicted token index: 55\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং \n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted token index: 7\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং ক\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 40\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃ\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted token index: 27\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicted token index: 2\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 20\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্ট\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 4\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টি\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted token index: 3\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicted token index: 55\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির \n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted token index: 10\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির স\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicted token index: 48\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌ\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted token index: 10\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicted token index: 2\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস্\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted token index: 7\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস্ক\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicted token index: 55\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস্ক \n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Predicted token index: 62\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস্ক ঐ\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicted token index: 9\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস্ক ঐত\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Predicted token index: 4\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস্ক ঐতি\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted token index: 24\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস্ক ঐতিহ\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicted token index: 1\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস্ক ঐতিহা\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicted token index: 7\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস্ক ঐতিহাক\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicted token index: 4\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস্ক ঐতিহাকি\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Predicted token index: 7\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস্ক ঐতিহাকিক\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 4\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস্ক ঐতিহাকিকি\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted token index: 55\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস্ক ঐতিহাকিকি \n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 8\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস্ক ঐতিহাকিকি ব\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Predicted token index: 1\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস্ক ঐতিহাকিকি বা\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicted token index: 22\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস্ক ঐতিহাকিকি বাজ\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Predicted token index: 55\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস্ক ঐতিহাকিকি বাজ \n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 24\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস্ক ঐতিহাকিকি বাজ হ\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Predicted token index: 4\n",
            "Decoded so far:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস্ক ঐতিহাকিকি বাজ হি\n",
            "Response:  আপনি কি বিভানে ন্রববান ব্যবার কর্ছে এবং এটি একটি সুম্যান কর্পারে এবং কৃষ্টির সৌস্ক ঐতিহাকিকি বাজ হি\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the trained model\n",
        "model = tf.keras.models.load_model('bangla_chatbot_model.keras2')\n",
        "\n",
        "# Model Inference Setup\n",
        "# Encoder Inference\n",
        "encoder_model = tf.keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Decoder Inference\n",
        "decoder_state_input_h = tf.keras.Input(shape=(512,))\n",
        "decoder_state_input_c = tf.keras.Input(shape=(512,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = tf.keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "\n",
        "# Ensure start token is in the tokenizer's word index and its index is valid\n",
        "start_token = '\\t'\n",
        "if start_token in tokenizer.word_index:\n",
        "    start_token_index = tokenizer.word_index[start_token]\n",
        "    if start_token_index >= vocab_size:\n",
        "        raise ValueError(\"Start token index exceeds vocabulary size.\")\n",
        "else:\n",
        "    raise ValueError(\"Start token not found in tokenizer's word index.\")\n",
        "\n",
        "# Function to generate response\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = tokenizer.word_index['\\t']  # Start token\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    iteration_count = 0\n",
        "    max_iterations = 100  # Set a limit to iterations for debugging\n",
        "\n",
        "\n",
        "    while not stop_condition and iteration_count < max_iterations:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        print(f\"Predicted token index: {sampled_token_index}\")  # Debug print\n",
        "\n",
        "\n",
        "        if sampled_token_index != 0:  # Skip padding token\n",
        "            sampled_char = tokenizer.index_word.get(sampled_token_index, '?')  # Use '?' for unknown indices\n",
        "            decoded_sentence += sampled_char\n",
        "\n",
        "        print(f\"Decoded so far: {decoded_sentence}\")\n",
        "\n",
        "        if sampled_char == '\\n' or len(decoded_sentence) > max_len_answers:\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "        iteration_count += 1\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "# Example usage\n",
        "# Convert the input question to sequence\n",
        "input_question = input()\n",
        "print(input_question)\n",
        "input_seq = tokenizer.texts_to_sequences([input_question])\n",
        "input_seq = pad_sequences(input_seq, maxlen=max_len_questions, padding='post')\n",
        "\n",
        "# Debug tokenization\n",
        "print(\"Tokenized sequence:\", input_seq)\n",
        "\n",
        "# # Check what token 54 represents\n",
        "# print(\"Token 54 corresponds to:\", tokenizer.index_word[54])\n",
        "\n",
        "# Generate response\n",
        "response = decode_sequence(input_seq)\n",
        "print(\"Response:\", response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **FAILED ATTEMPT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def decode_sequence(input_seq):\n",
        "#     # Encode the input sequence to get the internal state\n",
        "#     states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "#     # Generate empty target sequence of length 1 with only the start character\n",
        "#     target_seq = np.zeros((1, 1))\n",
        "#     #target_seq[0, 0] = tokenizer.word_index['\\t']   # '\\t' is the start token for the response\n",
        "#     target_seq[0, 0] = start_token_index  \n",
        "\n",
        "#     # Loop for generating response\n",
        "#     stop_condition = False\n",
        "#     decoded_sentence = ''\n",
        "#     while not stop_condition:\n",
        "#         output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "#         # Sample a token\n",
        "#         sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "#         sampled_char = tokenizer.index_word[sampled_token_index]\n",
        "#         decoded_sentence += sampled_char\n",
        "\n",
        "#         # Exit condition: either hit max length or find stop token.\n",
        "#         if sampled_char == '\\n' or len(decoded_sentence) > max_len_answers:\n",
        "#             stop_condition = True\n",
        "\n",
        "#         # Update the target sequence (length 1).\n",
        "#         target_seq = np.zeros((1, 1))\n",
        "#         target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "#         # Update internal states\n",
        "#         states_value = [h, c]\n",
        "\n",
        "#     return decoded_sentence\n",
        "\n",
        "# def decode_sequence(input_seq):\n",
        "#     # Encode the input sequence to get the internal state\n",
        "#     states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "#     # Generate empty target sequence of length 1 with only the start character\n",
        "#     target_seq = np.zeros((1, 1))\n",
        "#     target_seq[0, 0] = tokenizer.word_index['\\t']  # '\\t' is the start token for the response\n",
        "\n",
        "#     # Loop for generating response\n",
        "#     stop_condition = False\n",
        "#     decoded_sentence = ''\n",
        "#     while not stop_condition:\n",
        "#         output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "#         # Sample a token\n",
        "#         sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "#         # Check if the sampled token index is 0 (padding token)\n",
        "#         if sampled_token_index != 0:\n",
        "#             sampled_char = tokenizer.index_word[sampled_token_index]\n",
        "#             decoded_sentence += sampled_char\n",
        "\n",
        "#         # Exit condition: either hit max length or find stop token.\n",
        "#         if sampled_char == '\\n' or len(decoded_sentence) > max_len_answers:\n",
        "#             stop_condition = True\n",
        "\n",
        "#         # Update the target sequence (length 1).\n",
        "#         target_seq = np.zeros((1, 1))\n",
        "#         target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "#         # Update internal states\n",
        "#         states_value = [h, c]\n",
        "\n",
        "#     return decoded_sentence\n",
        "\n",
        "# def decode_sequence(input_seq):\n",
        "#     states_value = encoder_model.predict(input_seq)\n",
        "#     target_seq = np.zeros((1, 1))\n",
        "#     target_seq[0, 0] = tokenizer.word_index['\\t']  # Start token\n",
        "\n",
        "#     stop_condition = False\n",
        "#     decoded_sentence = ''\n",
        "#     sampled_char = ''  # Initialize sampled_char\n",
        "\n",
        "#     while not stop_condition:\n",
        "#         output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "#         sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "#         if sampled_token_index != 0:  # Skip padding token\n",
        "#             sampled_char = tokenizer.index_word[sampled_token_index]\n",
        "#             decoded_sentence += sampled_char\n",
        "\n",
        "#         print(f\"Decoded so far: {decoded_sentence}\")  # Debug print\n",
        "\n",
        "#         if sampled_char == '\\n' or len(decoded_sentence) > max_len_answers:\n",
        "#             stop_condition = True\n",
        "\n",
        "#         target_seq = np.zeros((1, 1))\n",
        "#         target_seq[0, 0] = sampled_token_index\n",
        "#         states_value = [h, c]\n",
        "\n",
        "#     return decoded_sentence\n",
        "\n",
        "# I noticed one thing: every time I retrain the model, the model accuracy goes down a bit. When I first trained the model, the accuracy was 0.8756, and now, after retraining the model a fourth time, the accuracy has gone down to 0.8474. Does this mean the model is not trained properly, and is this the reason that, while generating responses, the model entered an infinite loop?\n",
        "# Should I provide you with the whole code, from preprocessing datasets to training the model and generating responses, so that you can analyze the code and help me debug it?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
